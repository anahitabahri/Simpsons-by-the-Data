library(ggplot2) # data viz!
library(ggthemes) # data viz!
library(sqldf) # incorporate sql queries when dplyr doesn't work out
characters <- read_csv("data/simpsons_characters.csv")
episodes <- read_csv("data/simpsons_episodes.csv")
locations <- read_csv("data/simpsons_locations.csv")
lines <- read_csv("data/simpsons_script_lines.csv")
colnames(lines) # we care about character_id
colnames(characters) # we care about id
# using dplyr's left_join, join all the datasets!
simpsons_data <- left_join(lines, locations, by = c("location_id" = "id"))
simpsons_data <- left_join(simpsons_data, episodes, by = c("episode_id" = "id"))
simpsons_data <- left_join(simpsons_data, characters, by = c("character_id" = "id"))
colnames(simpsons_data)
# there are way too many cols here. let's trim it down to a select few.
simpsons_data <- select(simpsons_data, id, episode_id, number, timestamp_in_ms, speaking_line, raw_character_text, raw_location_text, normalized_text, word_count, title, original_air_date, season, number_in_season, number_in_series, us_viewers_in_millions, views, imdb_rating, gender)
class(simpsons_data$word_count)
# need to change data type of word count to numeric
simpsons_data$word_count <- as.numeric(simpsons_data$word_count)
# check other relevant data types
class(simpsons_data$us_viewers_in_millions)
class(simpsons_data$views)
class(simpsons_data$imdb_rating)
# use dplyr to get sum(word_count) and distinct(episode_id) grouped by characters
simpsons_data %>%
filter(speaking_line == "true") %>%
group_by(raw_character_text) %>%
summarise(word_count = sum(word_count, na.rm = TRUE),
ep_count = n_distinct(episode_id)) %>%
top_n(n = 10, wt = word_count) %>%
ggplot +
geom_bar(aes(raw_character_text, word_count), stat = "identity") +
coord_flip() +
labs(x = "Character", y = "Total Word Count") +
ggtitle("The Simpsons Top 10 Characters by Number of Spoken Words") +
theme_tws_simpsons()
# ggsave("charts/Top10Characters.png",dpi = 500)
simpsons_data %>%
filter(speaking_line == "true") %>%
group_by(raw_character_text) %>%
summarise(word_count = sum(word_count, na.rm = TRUE),
ep_count = n_distinct(episode_id)) %>%
top_n(n = 10, wt = word_count) %>%
ggplot +
geom_bar(aes(raw_character_text, word_count), stat = "identity") +
coord_flip() +
labs(x = "Character", y = "Total Word Count") +
ggtitle("The Simpsons Top 10 Characters by Number of Spoken Words") +
theme_tws_simpsons(base_size = 24, grid_width = 0.4)
# use dplyr to get sum(word_count) and distinct(episode_id) grouped by characters
simpsons_data %>%
filter(speaking_line == "true") %>%
group_by(raw_character_text) %>%
summarise(word_count = sum(word_count, na.rm = TRUE),
ep_count = n_distinct(episode_id)) %>%
top_n(n = 10, wt = word_count) %>%
ggplot +
geom_bar(aes(raw_character_text, word_count), stat = "identity") +
coord_flip() +
labs(x = "Character", y = "Total Word Count") +
ggtitle("The Simpsons Top 10 Characters by Number of Spoken Words") +
theme_tws_simpsons(base_size = 24, grid_width = 0.4)+heme(
panel.grid.major.y = element_blank(),
panel.grid.minor.y = element_blank(),
plot.title = element_text(size = 48, hjust = 4.4),
plot.subtitle = element_text(hjust = 1.38, margin = unit(c(0.25, 0.25, 2, 0.25), "line")),
axis.text.y = element_text(size = 24, margin = unit(c(0, -1, 0, 0), "line")),
axis.title.y = element_blank(),
axis.title.x = element_text(size = 40),
legend.position = "none"
)
# ggsave("charts/Top10Characters.png",dpi = 500)
library(tidyr) # tidy data
library(dplyr) # data manipulation
library(readr) # read files
library(ggplot2) # data viz!
library(ggthemes) # data viz!
library(sqldf) # incorporate sql queries when dplyr doesn't work out
characters <- read_csv("data/simpsons_characters.csv")
episodes <- read_csv("data/simpsons_episodes.csv")
locations <- read_csv("data/simpsons_locations.csv")
lines <- read_csv("data/simpsons_script_lines.csv")
library(tidyr) # tidy data
library(dplyr) # data manipulation
library(readr) # read files
library(ggplot2) # data viz!
library(ggthemes) # data viz!
library(sqldf) # incorporate sql queries when dplyr doesn't work out
characters <- read_csv("data/simpsons_characters.csv")
episodes <- read_csv("data/simpsons_episodes.csv")
locations <- read_csv("data/simpsons_locations.csv")
lines <- read_csv("data/simpsons_script_lines.csv")
setwd("~/Documents/MSSP/FA2016/Other/Simpsons-by-the-Data/684-Modeling")
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyr)
library(dplyr)
library(readr)
library(ggplot2)
library(sqldf)
library(MASS)
library(arm)
library(wordcloud)
library(SnowballC)
library(tm)
library(RColorBrewer)
library(data.table)
library(foreign)
library(car)
library(stringr)
library(RSentiment)
library(DT)
library(xtable)
characters <- read_csv("data/simpsons_characters.csv")
episodes <- read_csv("data/simpsons_episodes.csv")
locations <- read_csv("data/simpsons_locations.csv")
lines <- read_csv("data/simpsons_script_lines.csv")
# using dplyr's left_join, join all the datasets
simpsons_data <- left_join(lines, locations, by = c("location_id" = "id"))
simpsons_data <- left_join(simpsons_data, episodes, by = c("episode_id" = "id"))
simpsons_data <- left_join(simpsons_data, characters, by = c("character_id" = "id"))
# there are way too many columns. let's trim it down.
simpsons_data <-  dplyr::select(simpsons_data, id, episode_id, number, timestamp_in_ms, speaking_line, raw_character_text, raw_location_text, normalized_text, word_count, title, original_air_date, season, number_in_season, number_in_series, us_viewers_in_millions, views, imdb_rating, gender)
simpsons_data$word_count <- as.numeric(simpsons_data$word_count)
# convert gender to binary variable
simpsons_data$female <- ifelse(simpsons_data$gender=="f",1,ifelse(simpsons_data$gender=="m",0,"NA"))
simpsons_data$female <- as.numeric(simpsons_data$female)
# convert rating to binary (research shows 7 and above)
simpsons_data$rating <- ifelse(simpsons_data$imdb_rating >= 7,1,0)
fg_episodes <- read_csv("data/family_guy_episodes.csv")
fg_episodes <- na.omit(fg_episodes)
episodes_subset <- sqldf("SELECT episode_id, title, original_air_date, season, number_in_season, number_in_series, us_viewers_in_millions, views, imdb_rating, SUM(word_count) as sum_word_count, rating, COUNT(DISTINCT raw_location_text) AS location_count
FROM simpsons_data
GROUP BY episode_id")
character_subset_NA <- sqldf("SELECT raw_character_text, female, SUM(word_count) as sum_word_count, COUNT(DISTINCT episode_id) AS episode_count, COUNT(DISTINCT raw_location_text) AS location_count
FROM simpsons_data
GROUP BY raw_character_text
ORDER BY 3 DESC, raw_character_text")
character_subset <- na.omit(character_subset_NA)
character_subset$female <- as.integer(character_subset$female)
words_by_character <- sqldf("SELECT raw_character_text, gender, SUM(word_count) as sum_word_count, COUNT(DISTINCT episode_id) AS episode_count, female
FROM simpsons_data
WHERE gender IS NOT \"NA\"
GROUP BY raw_character_text
ORDER BY 3 DESC, raw_character_text")
xtable(lm(us_viewers_in_millions ~ original_air_date, data=simpsons_data))
ncvTest(rating_fit_1)
rating_fit_1 <- lm(imdb_rating ~ us_viewers_in_millions + sum_word_count + number_in_series, data=episodes_subset)
ncvTest(rating_fit_1)
xtable(ncvTest(rating_fit_1))
xtable(summary(rating_fit_1))
summary(rating_fit_1)
display(rating_fit_1)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyr)
library(dplyr)
library(readr)
library(ggplot2)
library(sqldf)
library(MASS)
library(arm)
library(wordcloud)
library(SnowballC)
library(tm)
library(RColorBrewer)
library(data.table)
library(foreign)
library(car)
library(stringr)
library(RSentiment)
library(DT)
# library(xtable)
characters <- read_csv("data/simpsons_characters.csv")
episodes <- read_csv("data/simpsons_episodes.csv")
locations <- read_csv("data/simpsons_locations.csv")
lines <- read_csv("data/simpsons_script_lines.csv")
# using dplyr's left_join, join all the datasets
simpsons_data <- left_join(lines, locations, by = c("location_id" = "id"))
simpsons_data <- left_join(simpsons_data, episodes, by = c("episode_id" = "id"))
simpsons_data <- left_join(simpsons_data, characters, by = c("character_id" = "id"))
# there are way too many columns. let's trim it down.
simpsons_data <-  dplyr::select(simpsons_data, id, episode_id, number, timestamp_in_ms, speaking_line, raw_character_text, raw_location_text, normalized_text, word_count, title, original_air_date, season, number_in_season, number_in_series, us_viewers_in_millions, views, imdb_rating, gender)
simpsons_data$word_count <- as.numeric(simpsons_data$word_count)
# convert gender to binary variable
simpsons_data$female <- ifelse(simpsons_data$gender=="f",1,ifelse(simpsons_data$gender=="m",0,"NA"))
simpsons_data$female <- as.numeric(simpsons_data$female)
# convert rating to binary (research shows 7 and above)
simpsons_data$rating <- ifelse(simpsons_data$imdb_rating >= 7,1,0)
fg_episodes <- read_csv("data/family_guy_episodes.csv")
fg_episodes <- na.omit(fg_episodes)
episodes_subset <- sqldf("SELECT episode_id, title, original_air_date, season, number_in_season, number_in_series, us_viewers_in_millions, views, imdb_rating, SUM(word_count) as sum_word_count, rating, COUNT(DISTINCT raw_location_text) AS location_count
FROM simpsons_data
GROUP BY episode_id")
character_subset_NA <- sqldf("SELECT raw_character_text, female, SUM(word_count) as sum_word_count, COUNT(DISTINCT episode_id) AS episode_count, COUNT(DISTINCT raw_location_text) AS location_count
FROM simpsons_data
GROUP BY raw_character_text
ORDER BY 3 DESC, raw_character_text")
character_subset <- na.omit(character_subset_NA)
character_subset$female <- as.integer(character_subset$female)
words_by_character <- sqldf("SELECT raw_character_text, gender, SUM(word_count) as sum_word_count, COUNT(DISTINCT episode_id) AS episode_count, female
FROM simpsons_data
WHERE gender IS NOT \"NA\"
GROUP BY raw_character_text
ORDER BY 3 DESC, raw_character_text")
rating_fit_1 <- lm(imdb_rating ~ us_viewers_in_millions + sum_word_count + number_in_series, data=episodes_subset)
display(rating_fit_1)
ktable(display(rating_fit_1))
library(xtable)
xtable(arm::display(rating_fit_1))
arm::display(rating_fit_1)
summary(rating_fit_1)
arm::display(rating_fit_1)
rating_fit_2 <- lm(imdb_rating ~ us_viewers_in_millions + sum_word_count + location_count, data=episodes_subset)
arm::display(rating_fit_2)
log_sum_word <- log(episodes_subset$sum_word_count)
log_number_series <- log(episodes_subset$number_in_series)
rating_fit_3 <- lm(imdb_rating ~ us_viewers_in_millions + log_sum_word + log_number_series, data=episodes_subset)
arm::display(rating_fit_2)
rating_fit_3 <- lm(imdb_rating ~ us_viewers_in_millions + log_sum_word + log_number_series, data=episodes_subset)
arm::display(rating_fit_2)
arm::display(rating_fit_3)
log_us_viewers <- log(episodes_subset$us_viewers_in_millions)
rating_fit_3 <- lm(imdb_rating ~ log_us_viewers + log_sum_word + log_number_series, data=episodes_subset)
arm::display(rating_fit_3)
rating_fit_4 <- lm(imdb_rating ~ log_us_viewers + log_sum_word + log_number_series, log_us_viewers:log_sum_word, data=episodes_subset)
arm::display(rating_fit_4)
rating_fit_4 <- lm(imdb_rating ~ log_us_viewers + log_sum_word + log_number_series, log_us_viewers:log_sum_word:log_number_series, data=episodes_subset)
arm::display(rating_fit_4)
summary(rating_fit_4)
rating_fit_4 <- lm(imdb_rating ~ log_us_viewers + log_sum_word + log_number_series, log_us_viewers:log_sum_word, data=episodes_subset)
arm::display(rating_fit_4)
summary(rating_fit_4)
xtable(arm::display(rating_fit_1))
arm::display(rating_fit_4)
rating_fit_5 <- glm(rating ~ us_viewers_in_millions + sum_word_count + number_in_series, family=binomial(link="logit"), data=episodes_subset)
arm:display(rating_fit_5)
arm::display(rating_fit_5)
invlogit(5.02)
invlogit(-3.41)
arm::display(rating_fit_5)
rating_fit_6 <- glm(rating ~ us_viewers_in_millions + number_in_series + location_count, family=binomial(link="logit"), data=episodes_subset)
arm::display(rating_fit_6)
arm::display(rating_fit_5)
rating_fit <- glm(rating ~ us_viewers_in_millions, family=binomial(link="logit"), data=episodes_subset)
arm::display(rating_fit)
invlogit(-1.47)
ggplot(data=episodes_subset, aes(x=us_viewers_in_millions, y=rating)) + geom_jitter(position=position_jitter(height=.15)) + geom_smooth(method="glm", method.args = list(family = "binomial")) +
ggtitle("Fitted Model") +
xlab("US Viewers in Millions") + ylab("Pr(Getting Good Rating)")
ggplot(data=episodes_subset, aes(x=us_viewers_in_millions, y=rating)) + geom_jitter(position=position_jitter(height=.15)) + geom_smooth(method="glm", method.args = list(family = "binomial"), color="#E77471") +
ggtitle("Fitted Model") +
xlab("US Viewers in Millions") + ylab("Pr(Getting Good Rating)")
plot(predict(rating_fit),residuals(rating_fit), main="Residual plot", xlab="Estimated Pr (Getting Good Rating)", ylab="Observed Estimated")
binnedplot(predict(rating_fit),residuals(rating_fit),main="Binned Residual Plot", xlab="Estimated Pr (Getting Good Rating)", ylab="Average Residual")
plot(predict(rating_fit),residuals(rating_fit), main="Residual plot", xlab="Estimated Pr (Getting Good Rating)", ylab="Observed Estimated")
binnedplot(predict(rating_fit),residuals(rating_fit),main="Binned Residual Plot", xlab="Estimated Pr (Getting Good Rating)", ylab="Average Residual")
plot(predict(rating_fit),residuals(rating_fit), main="Residual plot", xlab="Estimated Pr (Getting Good Rating)", ylab="Observed Estimated")
binnedplot(predict(rating_fit),residuals(rating_fit),main="Binned Residual Plot", xlab="Estimated Pr (Getting Good Rating)", ylab="Average Residual")
rating_fit_5 <- glm(rating ~ us_viewers_in_millions + sum_word_count + number_in_series, family=binomial(link="logit"), data=episodes_subset)
arm::display(rating_fit_5)
rating_fit_7 <- glm(rating ~ log_us_viewers + log_number_series + log_sum_word, family=binomial(link="logit"), data=episodes_subset)
arm::display(rating_fit_7)
arm::display(rating_fit_6)
rating_fit_8 <- glm(rating ~ log_us_viewers + log_number_series + log_sum_word, log_us_viewers:log_number_series, family=binomial(link="logit"), data=episodes_subset)
arm::display(rating_fit_8)
rating_fit_8 <- glm(rating ~ log_us_viewers + log_number_series + log_sum_word, log_us_viewers:log_sum_word, family=binomial(link="logit"), data=episodes_subset)
arm::display(rating_fit_8)
gender_fit_1 <- glm(female ~ sum_word_count + episode_count + location_count, family=binomial(link="logit"), data=character_subset)
arm::display(gender_fit_1)
arm::display(rating_fit_7)
rating_fit_6 <- glm(rating ~ us_viewers_in_millions + number_in_series + location_count, family=binomial(link="logit"), data=episodes_subset)
arm::display(rating_fit_6)
gender_fit_1 <- glm(female ~ sum_word_count + episode_count + location_count, family=binomial(link="logit"), data=character_subset)
arm::display(gender_fit_1)
invlogit(-1.20)
log_sum_word_count <- log(character_subset$sum_word_count)
gender_fit_2 <- glm(female ~ log_sum_word_count + episode_count + location_count, family=binomial(link="logit"), data=character_subset)
arm::display(gender_fit_2)
arm::display(gender_fit_1)
arm::display(gender_fit_2)
log_episode_count <- log(character_subset$episode_count)
log_location_count <- log(character_subset$location_count)
gender_fit_3 <- glm(female ~ log_sum_word_count + log_episode_count + log_location_count, family=binomial(link="logit"), data=character_subset)
xtable(summary(gender_fit_3))
arm::display(gender_fit_3)
arm::display(gender_fit_1)
simpsons_data$words <- ifelse(simpsons_data$normalized_text%in%c("love","please","right","help","good","great","like","well"),1,0)
gender_fit_4 <- glm(female ~ word_count + words, family=binomial(link="logit"), data=simpsons_data)
arm::display(gender_fit_4)
invlogit(-1)
kable(lm(us_viewers_in_millions ~ original_air_date, data=simpsons_data))
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyr)
library(dplyr)
library(readr)
library(ggplot2)
library(sqldf)
library(MASS)
library(arm)
library(wordcloud)
library(SnowballC)
library(tm)
library(RColorBrewer)
library(data.table)
library(foreign)
library(car)
library(stringr)
library(RSentiment)
library(DT)
characters <- read_csv("data/simpsons_characters.csv")
episodes <- read_csv("data/simpsons_episodes.csv")
locations <- read_csv("data/simpsons_locations.csv")
lines <- read_csv("data/simpsons_script_lines.csv")
# using dplyr's left_join, join all the datasets
simpsons_data <- left_join(lines, locations, by = c("location_id" = "id"))
simpsons_data <- left_join(simpsons_data, episodes, by = c("episode_id" = "id"))
simpsons_data <- left_join(simpsons_data, characters, by = c("character_id" = "id"))
# there are way too many columns. let's trim it down.
simpsons_data <-  dplyr::select(simpsons_data, id, episode_id, number, timestamp_in_ms, speaking_line, raw_character_text, raw_location_text, normalized_text, word_count, title, original_air_date, season, number_in_season, number_in_series, us_viewers_in_millions, views, imdb_rating, gender)
simpsons_data$word_count <- as.numeric(simpsons_data$word_count)
# convert gender to binary variable
simpsons_data$female <- ifelse(simpsons_data$gender=="f",1,ifelse(simpsons_data$gender=="m",0,"NA"))
simpsons_data$female <- as.numeric(simpsons_data$female)
# convert rating to binary (research shows 7 and above)
simpsons_data$rating <- ifelse(simpsons_data$imdb_rating >= 7,1,0)
episodes_subset <- sqldf("SELECT episode_id, title, original_air_date, season, number_in_season, number_in_series, us_viewers_in_millions, views, imdb_rating, SUM(word_count) as sum_word_count, rating, COUNT(DISTINCT raw_location_text) AS location_count
FROM simpsons_data
GROUP BY episode_id")
character_subset_NA <- sqldf("SELECT raw_character_text, female, SUM(word_count) as sum_word_count, COUNT(DISTINCT episode_id) AS episode_count, COUNT(DISTINCT raw_location_text) AS location_count
FROM simpsons_data
GROUP BY raw_character_text
ORDER BY 3 DESC, raw_character_text")
character_subset <- na.omit(character_subset_NA)
character_subset$female <- as.integer(character_subset$female)
words_by_character <- sqldf("SELECT raw_character_text, gender, SUM(word_count) as sum_word_count, COUNT(DISTINCT episode_id) AS episode_count, female
FROM simpsons_data
WHERE gender IS NOT \"NA\"
GROUP BY raw_character_text
ORDER BY 3 DESC, raw_character_text")
kable(lm(us_viewers_in_millions ~ original_air_date, data=simpsons_data))
sp_1 <- lm(us_viewers_in_millions ~ original_air_date, data=simpsons_data)
kable(display(sp_1))
kable(summary(sp_1), digits=2)
kable(summary(sp_1)$coef, digits=2)
sp_2 <- lm(imdb_rating ~ original_air_date, data=simpsons_data)
kable(summary(sp_1)$coef, digits=2)
sp_1 <- lm(us_viewers_in_millions ~ original_air_date, data=simpsons_data)
kable(summary(sp_1)$coef, digits=2)
sp_1 <- lm(us_viewers_in_millions ~ original_air_date, data=simpsons_data)
kable(summary(sp_1)$coef, digits=3)
sp_2 <- lm(imdb_rating ~ original_air_date, data=simpsons_data)
kable(summary(sp_2)$coef, digits=2)
kable(summary(sp_2)$coef, digits=3)
sp_3 <- lm(imdb_rating ~ us_viewers_in_millions, data=simpsons_data)
kable(summary(sp_3)$coef, digits=3)
sp_4 <- lm(imdb_rating ~ us_viewers_in_millions, data=fg_episodes)
fg_episodes <- read_csv("data/family_guy_episodes.csv")
fg_episodes <- na.omit(fg_episodes)
sp_4 <- lm(imdb_rating ~ us_viewers_in_millions, data=fg_episodes)
kable(summary(sp_4)$coef, digits=3)
rating_fit <- glm(rating ~ us_viewers_in_millions, family=binomial(link="logit"), data=episodes_subset)
kable(summary(rating_fit)$coef, digits=2)
invlogit(-1.47)
rating_fit_1 <- lm(imdb_rating ~ us_viewers_in_millions + sum_word_count + number_in_series, data=episodes_subset)
rating_fit_1 <- lm(imdb_rating ~ us_viewers_in_millions + sum_word_count + number_in_series, data=episodes_subset)
kable(summary(rating_fit_1)$coef, digits=2)
arm::display(rating_fit_1)
rating_fit_5 <- glm(rating ~ us_viewers_in_millions + sum_word_count + number_in_series, family=binomial(link="logit"), data=episodes_subset)
kable(summary(rating_fit_5)$coef, digits=3)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyr)
library(dplyr)
library(readr)
library(ggplot2)
library(sqldf)
library(MASS)
library(arm)
library(wordcloud)
library(SnowballC)
library(tm)
library(RColorBrewer)
library(data.table)
library(foreign)
library(car)
library(stringr)
library(RSentiment)
library(DT)
characters <- read_csv("data/simpsons_characters.csv")
episodes <- read_csv("data/simpsons_episodes.csv")
locations <- read_csv("data/simpsons_locations.csv")
lines <- read_csv("data/simpsons_script_lines.csv")
# using dplyr's left_join, join all the datasets
simpsons_data <- left_join(lines, locations, by = c("location_id" = "id"))
simpsons_data <- left_join(simpsons_data, episodes, by = c("episode_id" = "id"))
simpsons_data <- left_join(simpsons_data, characters, by = c("character_id" = "id"))
# there are way too many columns. let's trim it down.
simpsons_data <-  dplyr::select(simpsons_data, id, episode_id, number, timestamp_in_ms, speaking_line, raw_character_text, raw_location_text, normalized_text, word_count, title, original_air_date, season, number_in_season, number_in_series, us_viewers_in_millions, views, imdb_rating, gender)
simpsons_data$word_count <- as.numeric(simpsons_data$word_count)
# convert gender to binary variable
simpsons_data$female <- ifelse(simpsons_data$gender=="f",1,ifelse(simpsons_data$gender=="m",0,"NA"))
simpsons_data$female <- as.numeric(simpsons_data$female)
# convert rating to binary (research shows 7 and above)
simpsons_data$rating <- ifelse(simpsons_data$imdb_rating >= 7,1,0)
episodes_subset <- sqldf("SELECT episode_id, title, original_air_date, season, number_in_season, number_in_series, us_viewers_in_millions, views, imdb_rating, SUM(word_count) as sum_word_count, rating, COUNT(DISTINCT raw_location_text) AS location_count
FROM simpsons_data
GROUP BY episode_id")
character_subset_NA <- sqldf("SELECT raw_character_text, female, SUM(word_count) as sum_word_count, COUNT(DISTINCT episode_id) AS episode_count, COUNT(DISTINCT raw_location_text) AS location_count
FROM simpsons_data
GROUP BY raw_character_text
ORDER BY 3 DESC, raw_character_text")
character_subset <- na.omit(character_subset_NA)
character_subset$female <- as.integer(character_subset$female)
words_by_character <- sqldf("SELECT raw_character_text, gender, SUM(word_count) as sum_word_count, COUNT(DISTINCT episode_id) AS episode_count, female
FROM simpsons_data
WHERE gender IS NOT \"NA\"
GROUP BY raw_character_text
ORDER BY 3 DESC, raw_character_text")
ggplot(simpsons_data, aes(original_air_date,us_viewers_in_millions)) +
geom_point(color="#E77471",alpha=0.1) +
geom_smooth(method=lm,color="black") +
ggtitle("The Simpsons TV Viewers by Episode") +
labs(x = "Original Air Date", y = "US Viewers in Millions")
ggsave("charts/USViewersOriginalAirDateRegression.png",dpi = 500)
sp_1 <- lm(us_viewers_in_millions ~ original_air_date, data=simpsons_data)
kable(summary(sp_1)$coef, digits=3)
ggplot(simpsons_data, aes(original_air_date,us_viewers_in_millions)) +
geom_point(color="#E77471",alpha=0.1) +
geom_smooth(method=lm,color="black") +
ggtitle("The Simpsons TV Viewers by Episode") +
labs(x = "Original Air Date", y = "US Viewers in Millions")
ggsave("charts/USViewersOriginalAirDate_LinearRegression.png",dpi = 500)
sp_1 <- lm(us_viewers_in_millions ~ original_air_date, data=simpsons_data)
kable(summary(sp_1)$coef, digits=3)
ggplot(simpsons_data, aes(original_air_date,imdb_rating)) +
geom_point(color="#E77471",alpha=0.1) +
geom_smooth(method=lm, color="black") +
ggtitle("The Simpsons TV ratings by Episode") +
labs(x = "Original Air Date", y = "IMDb Rating")
ggsave("charts/IMDbRatingOriginalAirDate_LinearRegression.png",dpi = 500)
sp_2 <- lm(imdb_rating ~ original_air_date, data=simpsons_data)
kable(summary(sp_2)$coef, digits=3)
ggplot(simpsons_data, aes(us_viewers_in_millions,imdb_rating)) +
geom_point(color="#E77471",alpha=0.1) +
geom_smooth(method=lm, color="black") +
ggtitle("The Simpsons TV ratings by US Viewers") +
labs(x = "US Viewers in Millions", y = "IMDb Rating")
ggsave("charts/IMDbRatingUSViewers_LinearRegression.png",dpi = 500)
sp_3 <- lm(imdb_rating ~ us_viewers_in_millions, data=simpsons_data)
kable(summary(sp_3)$coef, digits=3)
fg_episodes <- read_csv("data/family_guy_episodes.csv")
fg_episodes <- na.omit(fg_episodes)
ggplot(fg_episodes, aes(us_viewers_in_millions,imdb_rating)) +
geom_point() +
geom_smooth(method=lm, color="#E77471") +
ggtitle("Family Guy TV ratings by US Viewers") +
labs(x = "US Viewers in Millions", y = "IMDb Rating")
ggsave("charts/IMDbRatingUSViewers_LinearRegression_FamilyGuy.png",dpi = 500)
sp_4 <- lm(imdb_rating ~ us_viewers_in_millions, data=fg_episodes)
kable(summary(sp_4)$coef, digits=2)
rating_fit <- glm(rating ~ us_viewers_in_millions, family=binomial(link="logit"), data=episodes_subset)
kable(summary(rating_fit)$coef, digits=2)
invlogit(-1.47)
ggplot(data=episodes_subset, aes(x=us_viewers_in_millions, y=rating)) + geom_jitter(position=position_jitter(height=.15)) + geom_smooth(method="glm", method.args = list(family = "binomial"), color="#E77471") +
ggtitle("Fitted Model") +
xlab("US Viewers in Millions") + ylab("Pr(Getting Good Rating)")
ggsave("charts/GoodIMDbRatingUSViewers_LogisticRegression.png",dpi = 500)
simpsons_data$words <- ifelse(simpsons_data$normalized_text%in%c("love","please","right","help","good","great","like","well"),1,0)
gender_fit_4 <- glm(female ~ word_count + words, family=binomial(link="logit"), data=simpsons_data)
arm::display(gender_fit_4)
invlogit(-1)
simpsons_words <- simpsons_data[,c("raw_character_text","raw_location_text","normalized_text","word_count")]
simpsons_words <- simpsons_words[!is.na(simpsons_words$word_count),]
simpsons_words$raw_location_text <- as.factor(simpsons_words$raw_location_text)
simpsons_words$raw_character_text <- as.factor(simpsons_words$raw_character_text)
family <- simpsons_words[simpsons_words$raw_character_text%in%c("Lisa Simpson","Bart Simpson","Homer Simpson","Marge Simpson","Maggie Simpson"),]
simpsons_words <- as.data.table(simpsons_words)
family <- as.data.table(family)
# homer simpson
homer <- family$normalized_text[family$raw_character_text=="Homer Simpson"]
corpus = Corpus(VectorSource(list(homer)))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeWords, stopwords('english'))
dtm_homer = DocumentTermMatrix(VCorpus(VectorSource(corpus[[1]]$content)))
freq_homer <- colSums(as.matrix(dtm_homer))
sentiments_homer = calculate_sentiment(names(freq_homer))
sentiments_homer = cbind(sentiments_homer, as.data.frame(freq_homer))
sent_pos_homer = sentiments_homer[sentiments_homer$sentiment == 'Positive',]
sent_neg_homer = sentiments_homer[sentiments_homer$sentiment == 'Negative',]
wordcloud(sent_pos_homer$text,sent_pos_homer$freq, min.freq=10,colors=brewer.pal(6,"Dark2"), main="Homer Simpson")
ggsave("charts/HomerSimpson_Wordcloud.png",dpi = 500)
# marge simpson
marge <- family$normalized_text[family$raw_character_text=="Marge Simpson"]
corpus = Corpus(VectorSource(list(marge)))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeWords, stopwords('english'))
dtm_marge = DocumentTermMatrix(VCorpus(VectorSource(corpus[[1]]$content)))
freq_marge <- colSums(as.matrix(dtm_marge))
sentiments_marge = calculate_sentiment(names(freq_marge))
sentiments_marge = cbind(sentiments_marge, as.data.frame(freq_marge))
sent_pos_marge = sentiments_marge[sentiments_marge$sentiment == 'Positive',]
sent_neg_marge = sentiments_marge[sentiments_marge$sentiment == 'Negative',]
wordcloud(sent_pos_marge$text,sent_pos_marge$freq, min.freq=10,colors=brewer.pal(6,"Dark2"), main="Marge Simpson")
ggsave("charts/MargeSimpson_Wordcloud.png",dpi = 500)
